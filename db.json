{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1}],"Cache":[{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1538311582595},{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1538311582589},{"_id":"themes/next/.gitignore","hash":"a18c2e83bb20991b899b58e6aeadcb87dd8aa16e","modified":1538311582599},{"_id":"themes/next/.eslintrc.json","hash":"cc5f297f0322672fe3f684f823bc4659e4a54c41","modified":1538311582619},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1538311582604},{"_id":"themes/next/LICENSE.md","hash":"fc7227c508af3351120181cbf2f9b99dc41f063e","modified":1538311582513},{"_id":"themes/next/README.md","hash":"140f4ece6670327a7d33b293947d958de80b44da","modified":1538311582599},{"_id":"themes/next/_config.yml","hash":"99e929258e23e37d3247b49af4786a5875c15508","modified":1538311582599},{"_id":"themes/next/crowdin.yml","hash":"e026078448c77dcdd9ef50256bb6635a8f83dca6","modified":1538311582595},{"_id":"themes/next/bower.json","hash":"5391684ba84fe633bc7877ab711a7d8a0072ceda","modified":1538311582589},{"_id":"themes/next/gulpfile.coffee","hash":"48d2f9fa88a4210308fc41cc7d3f6d53989f71b7","modified":1538311582546},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1538311582604},{"_id":"themes/next/.travis.yml","hash":"3d1dc928c4a97933e64379cfde749dedf62f252c","modified":1538311582619},{"_id":"themes/next/package.json","hash":"9acf6b8c8194f8a366025c8aab64d236908e62a1","modified":1538311582599},{"_id":"source/_posts/QANet-0.md","hash":"f9ca4c99f4826c691d435af391f7b978f5ccaf1e","modified":1538314275322},{"_id":"source/_posts/hello-world.md","hash":"8a02477044e2b77f1b262da2c48c01429e4a32e4","modified":1538310684249},{"_id":"themes/next/.git/config","hash":"e2ca9fa6f115d4406d24bf0df53fc26ce13e0c9b","modified":1538311582604},{"_id":"themes/next/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1538311582608},{"_id":"themes/next/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1538311582610},{"_id":"themes/next/.github/CODE_OF_CONDUCT.md","hash":"b63696d41f022525e40d7e7870c3785b6bc7536b","modified":1538311582603},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"a5335a99377069ae76fd993d488bc3eaf48f3a05","modified":1538311582604},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"00c25366764e6b9ccb40b877c60dc13b2916bbf7","modified":1538311582603},{"_id":"themes/next/.git/packed-refs","hash":"3cfda5adaa4c21b05b5697fd7ac88310f7d4a56f","modified":1538311582619},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"7abbb4c8a29b2c14e576a00f53dbc0b4f5669c13","modified":1538311582603},{"_id":"themes/next/.github/stale.yml","hash":"fd0856f6745db8bd0228079ccb92a662830cc4fb","modified":1538311582603},{"_id":"themes/next/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1538311582603},{"_id":"themes/next/.git/index","hash":"d634318f4d6e37fb9376bc0c5bd8e8138cc57257","modified":1538313402157},{"_id":"themes/next/docs/AGPL3.md","hash":"0d2b8c5fa8a614723be0767cc3bca39c49578036","modified":1538311582592},{"_id":"themes/next/docs/ALGOLIA-SEARCH.md","hash":"141e989844d0b5ae2e09fb162a280715afb39b0d","modified":1538311582590},{"_id":"themes/next/docs/DATA-FILES.md","hash":"8e1962dd3e1b700169b3ae5bba43992f100651ce","modified":1538311582590},{"_id":"themes/next/docs/AUTHORS.md","hash":"7b24be2891167bdedb9284a682c2344ec63e50b5","modified":1538311582589},{"_id":"themes/next/docs/INSTALLATION.md","hash":"2bbdd6c1751b2b42ce9b9335da420c6026a483e9","modified":1538311582595},{"_id":"themes/next/docs/LEANCLOUD-COUNTER-SECURITY.md","hash":"120750c03ec30ccaa470b113bbe39f3d423c67f0","modified":1538311582594},{"_id":"themes/next/docs/LICENSE","hash":"fe607fe22fc9308f6434b892a7f2d2c5514b8f0d","modified":1538311582590},{"_id":"themes/next/docs/MATH.md","hash":"0ae4258950de01a457ea8123a8d13ec6db496e53","modified":1538311582591},{"_id":"themes/next/docs/UPDATE-FROM-5.1.X.md","hash":"ad57c168d12ba01cf144a1ea0627b2ffd1847d3e","modified":1538311582589},{"_id":"themes/next/languages/de.yml","hash":"fb478c5040a4e58a4c1ad5fb52a91e5983d65a3a","modified":1538311582596},{"_id":"themes/next/languages/default.yml","hash":"c540c3a0d7db2d4239293c8783881962640b6c34","modified":1538311582596},{"_id":"themes/next/languages/en.yml","hash":"c540c3a0d7db2d4239293c8783881962640b6c34","modified":1538311582597},{"_id":"themes/next/languages/fr.yml","hash":"0162a85ae4175e66882a9ead1249fedb89200467","modified":1538311582598},{"_id":"themes/next/languages/id.yml","hash":"e7fb582e117a0785036dcdbb853a6551263d6aa6","modified":1538311582596},{"_id":"themes/next/languages/it.yml","hash":"62ef41d0a9a3816939cb4d93a524e6930ab9c517","modified":1538311582597},{"_id":"themes/next/languages/ja.yml","hash":"e331b15b1fda0f2285d25853f834682ab8dc3c39","modified":1538311582597},{"_id":"themes/next/languages/ko.yml","hash":"fae155018ae0efdf68669b2c7dd3f959c2e45cc9","modified":1538311582596},{"_id":"themes/next/languages/nl.yml","hash":"bb9ce8adfa5ee94bc6b5fac6ad24ba4605d180d3","modified":1538311582598},{"_id":"themes/next/languages/pt-BR.yml","hash":"bfc80c8a363fa2e8dde38ea2bc85cd19e15ab653","modified":1538311582596},{"_id":"themes/next/languages/pt.yml","hash":"3cb51937d13ff12fcce747f972ccb664840a9ef3","modified":1538311582596},{"_id":"themes/next/languages/ru.yml","hash":"db0644e738d2306ac38567aa183ca3e859a3980f","modified":1538311582597},{"_id":"themes/next/languages/tr.yml","hash":"c5f0c20743b1dd52ccb256050b1397d023e6bcd9","modified":1538311582598},{"_id":"themes/next/languages/vi.yml","hash":"8da921dd8335dd676efce31bf75fdd4af7ce6448","modified":1538311582598},{"_id":"themes/next/languages/zh-CN.yml","hash":"fbbf3a0b664ae8e927c700b0a813692b94345156","modified":1538311582598},{"_id":"themes/next/languages/zh-HK.yml","hash":"7903b96912c605e630fb695534012501b2fad805","modified":1538311582597},{"_id":"themes/next/languages/zh-TW.yml","hash":"6e6d2cd8f4244cb1b349b94904cb4770935acefd","modified":1538311582598},{"_id":"themes/next/layout/_layout.swig","hash":"e96796475d92aba0a4446efffadf79bf69a623a8","modified":1538313898043},{"_id":"themes/next/layout/archive.swig","hash":"2b6450c6b6d2bcbcd123ad9f59922a5e323d77a5","modified":1538311582531},{"_id":"themes/next/layout/category.swig","hash":"5d955284a42f802a48560b4452c80906a5d1da02","modified":1538311582517},{"_id":"themes/next/layout/index.swig","hash":"c2a3896c64e96790edc10426ef586b6186a87f46","modified":1538311582522},{"_id":"themes/next/layout/page.swig","hash":"79040bae5ec14291441b33eea341a24a7c0e9f93","modified":1538311582522},{"_id":"themes/next/layout/post.swig","hash":"318249db246a57e9422875a2457c6acfce974ba5","modified":1538311582522},{"_id":"themes/next/layout/schedule.swig","hash":"3e9cba5313bf3b98a38ccb6ef78b56ffa11d66ee","modified":1538311582516},{"_id":"themes/next/layout/tag.swig","hash":"ba402ce8fd55e80b240e019e8d8c48949b194373","modified":1538311582515},{"_id":"themes/next/scripts/helpers.js","hash":"a70bfad3efda76738dab12e28e8b75e3989ee3da","modified":1538311582602},{"_id":"themes/next/scripts/merge-configs.js","hash":"33afe97284d34542015d358a720823feeebef120","modified":1538311582600},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1538311582600},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1538311582514},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1538311582514},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1538311582514},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1538311582589},{"_id":"themes/next/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1538311582612},{"_id":"themes/next/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1538311582614},{"_id":"themes/next/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1538311582616},{"_id":"themes/next/.git/hooks/fsmonitor-watchman.sample","hash":"f7c0aa40cb0d620ff0bca3efe3521ec79e5d7156","modified":1538311582615},{"_id":"themes/next/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1538311582616},{"_id":"themes/next/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1538311582613},{"_id":"themes/next/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1538311582617},{"_id":"themes/next/.git/hooks/pre-rebase.sample","hash":"288efdc0027db4cfd8b7c47c4aeddba09b6ded12","modified":1538311582613},{"_id":"themes/next/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1538311582615},{"_id":"themes/next/.git/hooks/prepare-commit-msg.sample","hash":"2584806ba147152ae005cb675aa4f01d5d068456","modified":1538311582615},{"_id":"themes/next/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1538311582608},{"_id":"themes/next/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1538311582617},{"_id":"themes/next/.git/logs/HEAD","hash":"614b9e24d1eda268195d3cdc746ee19bb58f5467","modified":1538311582608},{"_id":"themes/next/docs/ru/DATA-FILES.md","hash":"d6d20f60f77a76c77f8e65d0c9adbd79d0274557","modified":1538311582591},{"_id":"themes/next/docs/ru/INSTALLATION.md","hash":"6c5d69e94961c793da156217ecf1179e868d7ba1","modified":1538311582591},{"_id":"themes/next/docs/ru/README.md","hash":"c54e256ed11a84ee38f755d6f35a3e6e29a91dbc","modified":1538311582591},{"_id":"themes/next/docs/ru/UPDATE-FROM-5.1.X.md","hash":"b1dd18d9b890b21718883ea1832e7e02a773104a","modified":1538311582591},{"_id":"themes/next/docs/zh-CN/ALGOLIA-SEARCH.md","hash":"6855402e2ef59aae307e8bd2a990647d3a605eb8","modified":1538311582593},{"_id":"themes/next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"a45a791b49954331390d548ac34169d573ea5922","modified":1538311582592},{"_id":"themes/next/docs/zh-CN/CONTRIBUTING.md","hash":"bd2c955d9b7b1b45bd74a4536717d547e03fcde3","modified":1538311582594},{"_id":"themes/next/docs/zh-CN/DATA-FILES.md","hash":"f3eec572a7d83542e2710a7404082014aaa1a5e7","modified":1538311582592},{"_id":"themes/next/docs/zh-CN/INSTALLATION.md","hash":"b19a6e0ae96eb7c756fb5b1ba03934c7f9cbb3c3","modified":1538311582594},{"_id":"themes/next/docs/zh-CN/LEANCLOUD-COUNTER-SECURITY.md","hash":"24cf2618d164440b047bb9396263de83bee5b993","modified":1538311582594},{"_id":"themes/next/docs/zh-CN/MATH.md","hash":"e03607b608db4aa7d46f6726827c51ac16623339","modified":1538311582593},{"_id":"themes/next/docs/zh-CN/README.md","hash":"aa6808f4f587c1a97205fa9427ba96a366bcb288","modified":1538311582593},{"_id":"themes/next/docs/zh-CN/UPDATE-FROM-5.1.X.md","hash":"5da70d7fa0c988a66a469b9795d33d471a4a4433","modified":1538311582592},{"_id":"themes/next/layout/_custom/head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1538311582517},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1538311582517},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1538311582516},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"31322a7f57936cf2dc62e824af5490da5354cf02","modified":1538311582546},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"0790ddbc349508d7ece45a9a4391d0a1cd7263cc","modified":1538311582545},{"_id":"themes/next/layout/_macro/post-related.swig","hash":"08fe30ce8909b920540231e36c97e28cfbce62b6","modified":1538311582544},{"_id":"themes/next/layout/_macro/post.swig","hash":"27922af64ecb9db9a28bcf1c98fb68b26bf0b67a","modified":1538311582544},{"_id":"themes/next/layout/_macro/reward.swig","hash":"bd5778d509c51f4b1d8da3a2bc35462929f08c75","modified":1538311582545},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"1f3121ef66a4698fd78f34bf2594ef79a407c92c","modified":1538311582544},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"a9e1346b83cf99e06bed59a53fc069279751e52a","modified":1538311582544},{"_id":"themes/next/layout/_partials/breadcrumb.swig","hash":"6994d891e064f10607bce23f6e2997db7994010e","modified":1538311582527},{"_id":"themes/next/layout/_partials/comments.swig","hash":"eafff2d623af8991844f34819a60e37ac11ef245","modified":1538311582527},{"_id":"themes/next/layout/_partials/footer.swig","hash":"05cdaf9b6cb32afd442b228cd247aaf9468a31ed","modified":1538311582527},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1aaf32bed57b976c4c1913fd801be34d4838cc72","modified":1538311582529},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"dbe321bcf3cf45917cc11a3e3f50d8572bac2c70","modified":1538311582523},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"0a0129e926c27fffc6e7ef87fe370016bc7a4564","modified":1538311582520},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"6fc63d5da49cb6157b8792f39c7305b55a0d1593","modified":1538311582521},{"_id":"themes/next/layout/_scripts/noscript.swig","hash":"ac3ad2c0eccdf16edaa48816d111aaf51200a54b","modified":1538311582520},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"e0bdc723d1dc858b41fd66e44e2786e6519f259f","modified":1538311582520},{"_id":"themes/next/layout/_third-party/bookmark.swig","hash":"60001c8e08b21bf3a7afaf029839e1455340e95d","modified":1538311582535},{"_id":"themes/next/layout/_third-party/copy-code.swig","hash":"a8ab2035654dd06d94faf11a35750529e922d719","modified":1538311582534},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"f532ce257fca6108e84b8f35329c53f272c2ce84","modified":1538311582535},{"_id":"themes/next/layout/_third-party/github-banner.swig","hash":"cabd9640dc3027a0b3ac06f5ebce777e50754065","modified":1538311582534},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"927f19160ae14e7030df306fc7114ba777476282","modified":1538311582539},{"_id":"themes/next/layout/_third-party/pangu.swig","hash":"6b75c5fd76ae7cf0a7b04024510bd5221607eab3","modified":1538311582540},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"fc93b1a7e6aed0dddb1f3910142b48d8ab61174e","modified":1538311582540},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1538311582534},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"b0ca46e0d1ff4c08cb0a3a8c1994f20d0260cef9","modified":1538311582535},{"_id":"themes/next/scripts/tags/button.js","hash":"4b12c376bea894d23cca0f9fcb3d6518b6db279d","modified":1538311582601},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"f13430d9d1c9773b390787c2f046bb1f12a79878","modified":1538311582601},{"_id":"themes/next/scripts/tags/exturl.js","hash":"1412ce2ef59fa4137b697a507fd759ff067a2398","modified":1538311582602},{"_id":"themes/next/scripts/tags/full-image.js","hash":"e282bf5a7c70b3d354001e8f66d3bef1a4fbb79e","modified":1538311582602},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"981e01aaf45a1f0f23ce0796d03134f9e437aaca","modified":1538311582602},{"_id":"themes/next/scripts/tags/include-raw.js","hash":"5db59d56f4f4082382bf1c16722e6c383892b0c5","modified":1538311582601},{"_id":"themes/next/scripts/tags/label.js","hash":"f0ecd3b5773b19a6bd93a819dfe0c49ee418e4de","modified":1538311582602},{"_id":"themes/next/scripts/tags/note.js","hash":"adb945ba93ac487d46b969ca4e59d3681b8f8d1c","modified":1538311582601},{"_id":"themes/next/scripts/tags/tabs.js","hash":"e37761253d68a29593fe9ed2fe403f49b6e971de","modified":1538311582601},{"_id":"themes/next/source/css/main.styl","hash":"c26ca6e7b5bd910b9046d6722c8e00be672890e0","modified":1538311582569},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1538311582579},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1538311582577},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1538311582578},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1538311582580},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1538311582578},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1538311582577},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1538311582577},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1538311582580},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1538311582579},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1538311582578},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1538311582579},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1538311582580},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1538311582579},{"_id":"themes/next/source/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1538311582580},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1538311582577},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1538311582578},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1538311582579},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1538311582578},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1538311582519},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1538311582519},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1538311582576},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1538311582576},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1538311582575},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1538311582568},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1538311582568},{"_id":"themes/next/.git/refs/heads/master","hash":"8f9e21b7efec2557ca7d3feb341aca3e00600d18","modified":1538311582618},{"_id":"themes/next/layout/_macro/menu/menu-badge.swig","hash":"65c5e585982dae7ae1542cada71858b4ea1f73d6","modified":1538311582545},{"_id":"themes/next/layout/_macro/menu/menu-item.swig","hash":"d1b73c926109145e52605929b75914cc8b60fb89","modified":1538311582545},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1538311582526},{"_id":"themes/next/layout/_partials/head/head-unique.swig","hash":"a7e376b087ae77f2e2a61ba6af81cde5af693174","modified":1538311582526},{"_id":"themes/next/layout/_partials/head/head.swig","hash":"00bf33b3c557b8f7e9faf49b226ea6ff7df5cda0","modified":1538311582526},{"_id":"themes/next/layout/_partials/header/menu.swig","hash":"3db735d0cd2d449edf2674310ac1e7c0043cb357","modified":1538311582528},{"_id":"themes/next/layout/_partials/header/brand.swig","hash":"fd780171713aada5eb4f4ffed8e714617c8ae6be","modified":1538311582529},{"_id":"themes/next/layout/_partials/header/sub-menu.swig","hash":"88b4b6051592d26bff59788acb76346ce4e398c2","modified":1538311582528},{"_id":"themes/next/layout/_partials/header/index.swig","hash":"2082f5077551123e695e8afec471c9c44b436acb","modified":1538311582529},{"_id":"themes/next/layout/_partials/search/index.swig","hash":"a33b29ccbdc2248aedff23b04e0627f435824406","modified":1538311582524},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1538311582525},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1538311582524},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1538311582525},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1538311582531},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1538311582530},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1538311582530},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"cc865af4a3cb6d25a0be171b7fc919ade306bb50","modified":1538311582521},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"ea03fe9c98ddcfcc0ecfdbe5a2b622f9cde3b3a1","modified":1538311582518},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"ea03fe9c98ddcfcc0ecfdbe5a2b622f9cde3b3a1","modified":1538311582519},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"98df9d72e37dd071e882f2d5623c9d817815b139","modified":1538311582542},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1538311582541},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1538311582542},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"07307f1f0e0e9858f2c7143cbdfcb2a9a92149ab","modified":1538311582541},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1538311582543},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"a234c5cd1f75ca5731e814d0dbb92fdcf9240d1b","modified":1538311582542},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"fae69a0e1a1d42f7bb44e594a29857d94594698b","modified":1538311582543},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"beb53371c035b62e1a2c7bb76c63afbb595fe6e5","modified":1538311582541},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"5e9bb24c750b49513d9a65799e832f07410002ac","modified":1538311582542},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"c28f3f4aa31d7f996d26a97df6cd7ffa9bfd2cec","modified":1538311582543},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1538311582542},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"0ddc94ed4ba0c19627765fdf1abc4d8efbe53d5a","modified":1538311582543},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1538311582540},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1538311582533},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"03ef008bc95e8e83232e5464a6c63d6157d33a5e","modified":1538311582532},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"fe8177e4698df764e470354b6acde8292a3515e0","modified":1538311582532},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"03e83f1311faafb7dddc2899042ed1cacd5c995e","modified":1538311582533},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"2c74a96dd314e804d801f8773ac1b2e0a970fce3","modified":1538311582533},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"34421679cae6581697cd3ab7c3729eb220e3e3f5","modified":1538311582533},{"_id":"themes/next/layout/_third-party/math/index.swig","hash":"a6fc00ec7f5642aabd66aa1cf51c6acc5b10e012","modified":1538311582536},{"_id":"themes/next/layout/_third-party/math/katex.swig","hash":"97dbc2035bcb5aa7eafb80a4202dc827cce34983","modified":1538311582536},{"_id":"themes/next/layout/_third-party/math/mathjax.swig","hash":"9b9ff4cc6d5474ab03f09835a2be80e0dba9fe89","modified":1538311582536},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1538311582537},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"b15e10abe85b4270860a56c970b559baa258b2a8","modified":1538311582538},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1538311582537},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1538311582539},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1538311582547},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"2aa5b7166a85a8aa34b17792ae4f58a5a96df6cc","modified":1538311582575},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"2640a54fa63bdd4c547eab7ce2fc1192cf0ccec8","modified":1538311582576},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"59961fb806a39c367fd19ad37268eee112be6729","modified":1538311582576},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"e1f6f59ad6e562dfe640ee4ed5d1ac9b6aba4114","modified":1538311582568},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"be087dcc060e8179f7e7f60ab4feb65817bd3d9f","modified":1538311582568},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"c167eeb6b736f7b021fba98c38c2c21032ee1255","modified":1538311582568},{"_id":"themes/next/source/css/_variables/base.styl","hash":"f9b83d0385529e52ce7ba95ed5ed6b3d4e2419bb","modified":1538311582568},{"_id":"themes/next/source/js/src/affix.js","hash":"a2aab233d99297435a5274bf512c3c753fe08e80","modified":1538311582583},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"1f7f10c579e7703d0f6acb8b73f3d78a07d0c623","modified":1538311582582},{"_id":"themes/next/source/js/src/exturl.js","hash":"54825acc8de4793feac415be227b965428f4e97d","modified":1538311582583},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"1c41508b83cb0c4512e64b4d63afa1be954ce8ef","modified":1538311582581},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1538311582582},{"_id":"themes/next/source/js/src/motion.js","hash":"b45d2c0d48f2c8e6a0621b8063845f76b89476cc","modified":1538311582583},{"_id":"themes/next/source/js/src/post-details.js","hash":"0dde5e6d4547587662a3256317a9d5d1db507692","modified":1538311582582},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"d07b3776708d4ae79ed2037c4c7391d5c9b06b19","modified":1538311582581},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fa3c92968bcdbcb8d95a1729f7659d9753cbd077","modified":1538311582581},{"_id":"themes/next/source/js/src/utils.js","hash":"e437eff1d3781c4a1aec9ff2060565524a37c983","modified":1538311582582},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1538311582587},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1538311582586},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1538311582587},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1538311582587},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1538311582584},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1538311582584},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1538311582584},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1538311582587},{"_id":"themes/next/.git/objects/pack/pack-2cbfd4769d5622783269e97c073ded1bf42ef495.idx","hash":"7342922f8f479c127d79cb12c6e1531997d6a313","modified":1538311582607},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1538311582584},{"_id":"themes/next/.git/logs/refs/heads/master","hash":"614b9e24d1eda268195d3cdc746ee19bb58f5467","modified":1538311582609},{"_id":"themes/next/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1538311582618},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"6958a97fde63e03983ec2394a4f8e408860fb42b","modified":1538311582538},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1538311582538},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"4719ce717962663c5c33ef97b1119a0b3a4ecdc3","modified":1538311582559},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"31050fc7a25784805b4843550151c93bfa55c9c8","modified":1538311582562},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"7e509c7c28c59f905b847304dd3d14d94b6f3b8e","modified":1538311582559},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1538311582559},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1538311582549},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"c5d48863f332ff8ce7c88dec2c893f709d7331d3","modified":1538311582559},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1538311582562},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"aebbd86500d819c4532ab290c62b6f432bc2f878","modified":1538311582567},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"b75256fe3768b1a37b6ff6dd7f9f0ff135a42067","modified":1538311582548},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1538311582547},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"efc40a32487e0ac7b94b1ca81bdbdcc4ec8f2924","modified":1538311582548},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1538311582548},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a280a583b7615e939aaddbf778f5c108ef8a2a6c","modified":1538311582549},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"02d138ed65060e98f20bc5b1dd59a791222b7156","modified":1538311582548},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"fca5320e2644edcd663888863899d1b80352439b","modified":1538311582571},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"0bef9f0dc134215bc4d0984ba3a16a1a0b6f87ec","modified":1538311582573},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1538311582574},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1538311582575},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"f43c821ea272f80703862260b140932fe4aa0e1f","modified":1538311582575},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"2212511ae14258d93bec57993c0385e5ffbb382b","modified":1538311582574},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1538311582574},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"5e12572b18846250e016a872a738026478ceef37","modified":1538311582573},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1538311582570},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1538311582570},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"35f093fe4c1861661ac1542d6e8ea5a9bbfeb659","modified":1538311582570},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1538311582570},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"d5e8ea6336bc2e237d501ed0d5bbcbbfe296c832","modified":1538311582570},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1538311582572},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"876b5d99061025cf485a3cac440624ded5734319","modified":1538311582571},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"05a5abf02e84ba8f639b6f9533418359f0ae4ecb","modified":1538311582572},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1538311582571},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"41f9cdafa00e256561c50ae0b97ab7fcd7c1d6a2","modified":1538311582572},{"_id":"themes/next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"ffa870c3fa37a48b01dc6f967e66f5df508d02bf","modified":1538311582572},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"5779cc8086b1cfde9bc4f1afdd85223bdc45f0a0","modified":1538311582571},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"58f87062210200c778eb92e20a9453bb6a5a03fa","modified":1538311582581},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1538311582587},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1538311582586},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1538311582586},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1538311582585},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1538311582585},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1538311582588},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1538311582588},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1538311582585},{"_id":"themes/next/.git/logs/refs/remotes/origin/HEAD","hash":"614b9e24d1eda268195d3cdc746ee19bb58f5467","modified":1538311582610},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"39dee82d481dd9d44e33658960ec63e47cd0a715","modified":1538311582553},{"_id":"themes/next/source/css/_common/components/header/github-banner.styl","hash":"ee37e6c465b9b2a7e39175fccfcbed14f2db039b","modified":1538311582566},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"7cc3f36222494c9a1325c5347d7eb9ae53755a32","modified":1538311582567},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1538311582567},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1538311582566},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1538311582566},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1538311582566},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1538311582559},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"a6dc3c7eb81ef5117c28fa2245fff1adc02d0292","modified":1538311582558},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1538311582558},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"f5aa2ba3bfffc15475e7e72a55b5c9d18609fdf5","modified":1538311582565},{"_id":"themes/next/source/css/_common/components/pages/breadcrumb.styl","hash":"7dd9a0378ccff3e4a2003f486b1a34e74c20dac6","modified":1538311582565},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1538311582565},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"fb451dc4cc0355b57849c27d3eb110c73562f794","modified":1538311582565},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1538311582565},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1538311582564},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"0f7f522cc6bfb3401d5afd62b0fcdf48bb2d604b","modified":1538311582557},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"e72a89e0f421444453e149ba32c77a64bd8e44e8","modified":1538311582556},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1538311582557},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1538311582557},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"ca89b167d368eac50a4f808fa53ba67e69cbef94","modified":1538311582558},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1538311582555},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"417f05ff12a2aaca6ceeac8b7e7eb26e9440c4c3","modified":1538311582554},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1538311582555},{"_id":"themes/next/source/css/_common/components/post/post-reading_progress.styl","hash":"f4e9f870baa56eae423a123062f00e24cc780be1","modified":1538311582556},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"549a8a0b5301d32acd86a97f17340cdfcd46fb63","modified":1538311582553},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"017074ef58166e2d69c53bb7590a0e7a8947a1ed","modified":1538311582554},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1538311582555},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"fcbbf06b546c366d70b7d2ba5880b0be3ca1e8ea","modified":1538311582555},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1538311582556},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"578bb2d5f24cad39205bbafb4c39c7e9962b9fa9","modified":1538311582557},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"6089cbf4c907fe198b6501e40dc937480d0be175","modified":1538311582554},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"35c0350096921dd8e2222ec41b6c17a4ea6b44f2","modified":1538311582550},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"e18b90c97aaff027e795f5a0cb10476a71bf1c3a","modified":1538311582551},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"89dd4f8b1f1cce3ad46cf2256038472712387d02","modified":1538311582550},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"efa5e5022e205b52786ce495d4879f5e7b8f84b2","modified":1538311582550},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1538311582552},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1538311582551},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"4427ed3250483ed5b7baad74fa93474bd1eda729","modified":1538311582551},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"f7784aba0c1cd20d824c918c120012d57a5eaa2a","modified":1538311582552},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"43bc58daa8d35d5d515dc787ceb21dd77633fe49","modified":1538311582552},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1538311582551},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1538311582562},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"6ec8ea7b11a146777b6b8da0f71f0cc1dbd129df","modified":1538311582560},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1538311582561},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"4a457d265d62f287c63d48764ce45d9bcfc9ec5a","modified":1538311582561},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"ee7528900578ef4753effe05b346381c40de5499","modified":1538311582561},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"32c9156bea5bac9e9ad0b4c08ffbca8b3d9aac4b","modified":1538311582561},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"4ab5deed8c3b0c338212380f678f8382672e1bcb","modified":1538311582560},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"5e340ee2407a4e39cd708794cfcc718a5f398d7b","modified":1538311582560},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"10e9bb3392826a5a8f4cabfc14c6d81645f33fe6","modified":1538311582563},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1538311582563},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"34935b40237c074be5f5e8818c14ccfd802b7439","modified":1538311582563},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"cce6772e2cdb4db85d35486ae4c6c59367fbdd40","modified":1538311582564},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"d89c4b562b528e4746696b2ad8935764d133bdae","modified":1538311582564},{"_id":"themes/next/source/css/_common/components/third-party/related-posts.styl","hash":"76937db9702053d772f6758d9cea4088c2a6e2a3","modified":1538311582563},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"a5e3e6b4b4b814a9fe40b34d784fed67d6d977fa","modified":1538311582564},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"1c18c91ab3c60169ebe654c80c968fd8458786a3","modified":1538311582563},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1538311582574},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1538311582573},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1538311582569},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1538311582588},{"_id":"themes/next/.git/objects/pack/pack-2cbfd4769d5622783269e97c073ded1bf42ef495.pack","hash":"2c2ad06de38e9577a8aa9ef7f3d3143327036233","modified":1538311582606}],"Category":[{"name":"NLP","_id":"cjmox84s4000224osbt8hns59"},{"name":"machine-reading-comprehension","parent":"cjmox84s4000224osbt8hns59","_id":"cjmox84s7000524osgnyj2ic5"}],"Data":[],"Page":[],"Post":[{"title":"QANet","date":"2018-09-30T10:01:32.000Z","_content":"\n## QANet\n#### QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension\n\n\n### 特点\n1. 移除RNN\n2. 使用卷积捕捉局部信息\n3. 自注意力机制捕捉全局信息\n\n### 模型结构\n<!--more-->\n![image](https://ws1.sinaimg.cn/large/006tNc79ly1fvrpc8wjfzj30rk0zw43g.jpg)\n1. Embedding层\n2. Embedding Encoder层\n3. Context-Query Attention层\n4. Model encoder层\n5. Output层\n\n#### Embedding层\n利用词向量和字向量拼接的方式获得最终的词向量：\n文章最大长度设为600\n- 词向量\n    > 预训练好的，300维的Glove\n    > embedding后维度为$600\\times300$\n- 字向量：\n    > 预处理时将每个单词截断或者是padding到16个字符\n    > 每个字符表示成200维的向量 （embedding后维度变成$600\\times16\\times200$）\n    > 做卷积filter=128,kernel=5 (卷积后维度为$600\\times12\\times128$)\n    > 沿行做最大池化，选择所有字符中最大的向量作为单词的最终字符向量。（维度变成$600\\times128$）\n    \n- 拼接，再经过两层Highway Network得到最终的单词向量表示，维度为$600\\times328$\n\n#### Embedding Encoder层\n提取Context和query中的信息\n每一个Encoder块是由卷积、Self-Attention、全连接层组成。\n输入向量维数是d=328，输出d=128.\n![image](https://ws2.sinaimg.cn/large/006tNc79ly1fvrpsx6edxj30k80xsmzy.jpg)\n**1. Position encoding**\n纯Attention模型无法捕捉序列的顺序，如果将K,V按行打乱顺序（相当于打乱句子中的次序），Attention的结果还是一样的。如果没有序列顺序信息，Attention模型顶多是一个非常精妙的“词袋模型”而已。Position Embedding是位置信息的唯一来源。\n\n$PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})$\n$PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})$\npos(i+k)可以表达成pos(i)的线性组合，对相对距离的表示非常有利\n把输入加上position embedding的结果作为输出\n\n**2. 深度可分离卷积**\n经典卷积：卷积核在所有输入通道上进行卷积，并综合所有输入通道情况得到卷积结果（池化）\n深度可分离卷积：卷积核对每个输入通道分别做卷积，然后对得到的新的feature map先concat起来再使用$1 \\times 1$的卷积核将输出通道混合。\n![image](https://ws1.sinaimg.cn/large/006tNc79ly1fvrpu79woaj310a0huq5v.jpg)\n![image](https://ws2.sinaimg.cn/large/006tNc79ly1fvrpulkoc4j30uy06gwfs.jpg)\n\n优点：把空间特征学习和通道特征学习分开，这样可以提高泛化能力和卷积效率，避免参数冗余。（Xception的基础）\n\n\n用3个$3\\times3$（1通道）的卷积核分别与输入的3通道的数据做卷积，得到3个feature map，然后用256个$1 \\times 1$大小的卷积核（3通道）在这3个feature map上进行卷积运算，将3个通道的信息进行融合。\n\n**3. 自注意力机制**\n[Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n序列编码：RNN要逐步递归才能获得全局信息，因此一般要双向RNN才比较好；CNN事实上只能获取局部信息，是通过层叠来增大感受野；Attention的思路最为粗暴，它一步到位获取了全局信息。\n![image](https://ws2.sinaimg.cn/large/006tNc79ly1fvrpvist54j30og0ss0uz.jpg)\n$Attention(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V}) = softmax\\left(\\frac{\\boldsymbol{Q}\\boldsymbol{K}^{\\top}}{\\sqrt{d_k}}\\right)\\boldsymbol{V}$\nquery和key做內积并softmax,来得到query和value之间的相似度，然后加权求和。\n\n**Self Attention**\n\n如果Q=K=V，那么就称为Self Attention，它的意思是直接将每个词与原来的每个词进行比较，计算出表示。也就是在序列内部做Attention，寻找序列内部的联系。\n\n**Muilti-Head Attention**\n\n这个是Google提出的新概念，是Attention机制的完善。不过从形式上看，它其实就再简单不过了，就是把Q,K,V通过参数矩阵映射一下，然后再做Attention，把这个过程重复做h次，结果拼接起来就行了\n![image](https://ws1.sinaimg.cn/large/006tNc79ly1fvrpwc63ldj30hc0kkq4k.jpg)\n\n所谓“多头”（Multi-Head），就是指多做几次同样的事情（参数不共享），然后把结果拼接。\n一方面，从直觉上多次attention操作可以捕获更多的信息，另一方面，先进行的投影操作能把QKV映射到不同空间，也许能发现更多特征。\n\n**4. 层归一化+残差连接**\n\n在encoder block中，每个子层都用了layernorm和残差连接：\n$Output = f(layernorm(x)) + x$\n其中 f 表示encoder block 中的子层，如 depth conv, self-attention, feed-forward等。layernorm() 表示 layer normalization。\n\n![image](https://ws3.sinaimg.cn/large/006tNc79ly1fvrpwq23s1j314m0cs0up.jpg)\nBN：针对一个minibatch的输入样本，计算均值和方差，基于计算的均值和方差来对某一层神经网络的输入X中每一个case进行归一化操作。\n\nLN: 同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；而BN中则针对不同神经元输入计算均值和方差，同一个minibatch中的输入拥有相同的均值和方差。因此，LN不依赖于mini-batch的大小和输入sequence的深度，\n\n\n#### Context-Query Attention Layer\n发现context query 之间的联系，并在词的层面上，解析出query, context中关键的词语。\n1. 首先计算context和query每对词之间的相似度，搞成一个相似度矩阵S，用的是BiDAF的算法：\n$S_{i,j} = f(q,c ) = W_0[q,c,q\\odot c]$\n2. 计算context-to-query的attention A：\n$A = softmax(S, axis=row) \\cdot Q^T \\quad \\in R^{n\\times d}$\n\n> 实现中，context的长度是600，question长度是100，经过embedding encoder之后分别变成$S=600\\times128$和$Q=100\\times128$的表示。$S\\cdot Q^T$维度是$600\\times100$\n\n> 每行是一个context中的单词w，元素值是所有的query单词对于当前文档单词w的注意力分配权值\n> 用$A$的每一行去乘以Q去表达单词w\n> 这样就得到了用query表达context的结果\n3. 计算query-to-context的attention B：\n$B = A\\cdot softmax(S,axis=column)^T \\cdot C^T$\n(这里采用了DCN的coattention) [Dynamic Coattention Networks For Question Answering](https://arxiv.org/abs/1611.01604)\n#### Model Encoder layer\n从全局的层面来考虑context与query之间的关系。\n输入是3个关于Context的矩阵信息：\n原始Context：$C \\in \\mathcal{R}^{n\\times d}$\nContext的Attention: $A \\in \\mathcal{R}^{n\\times d}$\nContext的Coattention:$B \\in \\mathcal{R}^{n \\times d}$ \n每个单词的编码信息是上面三个矩阵的拼接\n$f(w) = [c, a, c \\odot a, c \\odot b]$\n一个有7个Encoder-Block，每个Encoder-Block：2个卷积层、Self-Attention、FFN。其它参数和Embedding Encoder一样。\n一共有3个Model-Encoder，共享所有参数。输出依次为$M_0,M_1,M_2$.\n\n#### Output layer\n解析answer在context中的位置\n$pos^{start} = softmax(W_{start} [M_0; M_1]),\\quad  pos^{end} = softmax(W_{end}[M_0; M_2])$\n\n#### Loss function\n$L(\\theta) = -  \\frac{1}{N}\\sum_{i}^N\\left[\\log(p_{y_i^{start}}^{start}) + \\log(p_{y_i^{end}}^{end})\\right]$","source":"_posts/QANet-0.md","raw":"---\ntitle: QANet\ndate: 2018-09-30 18:01:32\ncategories:\n- NLP\n- machine-reading-comprehension\ntags:\n- NLP\n- machine-reading-comprehension\n---\n\n## QANet\n#### QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension\n\n\n### 特点\n1. 移除RNN\n2. 使用卷积捕捉局部信息\n3. 自注意力机制捕捉全局信息\n\n### 模型结构\n<!--more-->\n![image](https://ws1.sinaimg.cn/large/006tNc79ly1fvrpc8wjfzj30rk0zw43g.jpg)\n1. Embedding层\n2. Embedding Encoder层\n3. Context-Query Attention层\n4. Model encoder层\n5. Output层\n\n#### Embedding层\n利用词向量和字向量拼接的方式获得最终的词向量：\n文章最大长度设为600\n- 词向量\n    > 预训练好的，300维的Glove\n    > embedding后维度为$600\\times300$\n- 字向量：\n    > 预处理时将每个单词截断或者是padding到16个字符\n    > 每个字符表示成200维的向量 （embedding后维度变成$600\\times16\\times200$）\n    > 做卷积filter=128,kernel=5 (卷积后维度为$600\\times12\\times128$)\n    > 沿行做最大池化，选择所有字符中最大的向量作为单词的最终字符向量。（维度变成$600\\times128$）\n    \n- 拼接，再经过两层Highway Network得到最终的单词向量表示，维度为$600\\times328$\n\n#### Embedding Encoder层\n提取Context和query中的信息\n每一个Encoder块是由卷积、Self-Attention、全连接层组成。\n输入向量维数是d=328，输出d=128.\n![image](https://ws2.sinaimg.cn/large/006tNc79ly1fvrpsx6edxj30k80xsmzy.jpg)\n**1. Position encoding**\n纯Attention模型无法捕捉序列的顺序，如果将K,V按行打乱顺序（相当于打乱句子中的次序），Attention的结果还是一样的。如果没有序列顺序信息，Attention模型顶多是一个非常精妙的“词袋模型”而已。Position Embedding是位置信息的唯一来源。\n\n$PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})$\n$PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})$\npos(i+k)可以表达成pos(i)的线性组合，对相对距离的表示非常有利\n把输入加上position embedding的结果作为输出\n\n**2. 深度可分离卷积**\n经典卷积：卷积核在所有输入通道上进行卷积，并综合所有输入通道情况得到卷积结果（池化）\n深度可分离卷积：卷积核对每个输入通道分别做卷积，然后对得到的新的feature map先concat起来再使用$1 \\times 1$的卷积核将输出通道混合。\n![image](https://ws1.sinaimg.cn/large/006tNc79ly1fvrpu79woaj310a0huq5v.jpg)\n![image](https://ws2.sinaimg.cn/large/006tNc79ly1fvrpulkoc4j30uy06gwfs.jpg)\n\n优点：把空间特征学习和通道特征学习分开，这样可以提高泛化能力和卷积效率，避免参数冗余。（Xception的基础）\n\n\n用3个$3\\times3$（1通道）的卷积核分别与输入的3通道的数据做卷积，得到3个feature map，然后用256个$1 \\times 1$大小的卷积核（3通道）在这3个feature map上进行卷积运算，将3个通道的信息进行融合。\n\n**3. 自注意力机制**\n[Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n序列编码：RNN要逐步递归才能获得全局信息，因此一般要双向RNN才比较好；CNN事实上只能获取局部信息，是通过层叠来增大感受野；Attention的思路最为粗暴，它一步到位获取了全局信息。\n![image](https://ws2.sinaimg.cn/large/006tNc79ly1fvrpvist54j30og0ss0uz.jpg)\n$Attention(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V}) = softmax\\left(\\frac{\\boldsymbol{Q}\\boldsymbol{K}^{\\top}}{\\sqrt{d_k}}\\right)\\boldsymbol{V}$\nquery和key做內积并softmax,来得到query和value之间的相似度，然后加权求和。\n\n**Self Attention**\n\n如果Q=K=V，那么就称为Self Attention，它的意思是直接将每个词与原来的每个词进行比较，计算出表示。也就是在序列内部做Attention，寻找序列内部的联系。\n\n**Muilti-Head Attention**\n\n这个是Google提出的新概念，是Attention机制的完善。不过从形式上看，它其实就再简单不过了，就是把Q,K,V通过参数矩阵映射一下，然后再做Attention，把这个过程重复做h次，结果拼接起来就行了\n![image](https://ws1.sinaimg.cn/large/006tNc79ly1fvrpwc63ldj30hc0kkq4k.jpg)\n\n所谓“多头”（Multi-Head），就是指多做几次同样的事情（参数不共享），然后把结果拼接。\n一方面，从直觉上多次attention操作可以捕获更多的信息，另一方面，先进行的投影操作能把QKV映射到不同空间，也许能发现更多特征。\n\n**4. 层归一化+残差连接**\n\n在encoder block中，每个子层都用了layernorm和残差连接：\n$Output = f(layernorm(x)) + x$\n其中 f 表示encoder block 中的子层，如 depth conv, self-attention, feed-forward等。layernorm() 表示 layer normalization。\n\n![image](https://ws3.sinaimg.cn/large/006tNc79ly1fvrpwq23s1j314m0cs0up.jpg)\nBN：针对一个minibatch的输入样本，计算均值和方差，基于计算的均值和方差来对某一层神经网络的输入X中每一个case进行归一化操作。\n\nLN: 同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；而BN中则针对不同神经元输入计算均值和方差，同一个minibatch中的输入拥有相同的均值和方差。因此，LN不依赖于mini-batch的大小和输入sequence的深度，\n\n\n#### Context-Query Attention Layer\n发现context query 之间的联系，并在词的层面上，解析出query, context中关键的词语。\n1. 首先计算context和query每对词之间的相似度，搞成一个相似度矩阵S，用的是BiDAF的算法：\n$S_{i,j} = f(q,c ) = W_0[q,c,q\\odot c]$\n2. 计算context-to-query的attention A：\n$A = softmax(S, axis=row) \\cdot Q^T \\quad \\in R^{n\\times d}$\n\n> 实现中，context的长度是600，question长度是100，经过embedding encoder之后分别变成$S=600\\times128$和$Q=100\\times128$的表示。$S\\cdot Q^T$维度是$600\\times100$\n\n> 每行是一个context中的单词w，元素值是所有的query单词对于当前文档单词w的注意力分配权值\n> 用$A$的每一行去乘以Q去表达单词w\n> 这样就得到了用query表达context的结果\n3. 计算query-to-context的attention B：\n$B = A\\cdot softmax(S,axis=column)^T \\cdot C^T$\n(这里采用了DCN的coattention) [Dynamic Coattention Networks For Question Answering](https://arxiv.org/abs/1611.01604)\n#### Model Encoder layer\n从全局的层面来考虑context与query之间的关系。\n输入是3个关于Context的矩阵信息：\n原始Context：$C \\in \\mathcal{R}^{n\\times d}$\nContext的Attention: $A \\in \\mathcal{R}^{n\\times d}$\nContext的Coattention:$B \\in \\mathcal{R}^{n \\times d}$ \n每个单词的编码信息是上面三个矩阵的拼接\n$f(w) = [c, a, c \\odot a, c \\odot b]$\n一个有7个Encoder-Block，每个Encoder-Block：2个卷积层、Self-Attention、FFN。其它参数和Embedding Encoder一样。\n一共有3个Model-Encoder，共享所有参数。输出依次为$M_0,M_1,M_2$.\n\n#### Output layer\n解析answer在context中的位置\n$pos^{start} = softmax(W_{start} [M_0; M_1]),\\quad  pos^{end} = softmax(W_{end}[M_0; M_2])$\n\n#### Loss function\n$L(\\theta) = -  \\frac{1}{N}\\sum_{i}^N\\left[\\log(p_{y_i^{start}}^{start}) + \\log(p_{y_i^{end}}^{end})\\right]$","slug":"QANet-0","published":1,"updated":"2018-09-30T13:31:15.322Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjmox84rx000024os0usa6hjt","content":"<h2 id=\"QANet\"><a href=\"#QANet\" class=\"headerlink\" title=\"QANet\"></a>QANet</h2><h4 id=\"QANet-Combining-Local-Convolution-with-Global-Self-Attention-for-Reading-Comprehension\"><a href=\"#QANet-Combining-Local-Convolution-with-Global-Self-Attention-for-Reading-Comprehension\" class=\"headerlink\" title=\"QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension\"></a>QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension</h4><h3 id=\"特点\"><a href=\"#特点\" class=\"headerlink\" title=\"特点\"></a>特点</h3><ol>\n<li>移除RNN</li>\n<li>使用卷积捕捉局部信息</li>\n<li>自注意力机制捕捉全局信息</li>\n</ol>\n<h3 id=\"模型结构\"><a href=\"#模型结构\" class=\"headerlink\" title=\"模型结构\"></a>模型结构</h3><a id=\"more\"></a>\n<p><img src=\"https://ws1.sinaimg.cn/large/006tNc79ly1fvrpc8wjfzj30rk0zw43g.jpg\" alt=\"image\"></p>\n<ol>\n<li>Embedding层</li>\n<li>Embedding Encoder层</li>\n<li>Context-Query Attention层</li>\n<li>Model encoder层</li>\n<li>Output层</li>\n</ol>\n<h4 id=\"Embedding层\"><a href=\"#Embedding层\" class=\"headerlink\" title=\"Embedding层\"></a>Embedding层</h4><p>利用词向量和字向量拼接的方式获得最终的词向量：<br>文章最大长度设为600</p>\n<ul>\n<li>词向量<blockquote>\n<p>预训练好的，300维的Glove<br>embedding后维度为$600\\times300$</p>\n</blockquote>\n</li>\n<li><p>字向量：</p>\n<blockquote>\n<p>预处理时将每个单词截断或者是padding到16个字符<br>每个字符表示成200维的向量 （embedding后维度变成$600\\times16\\times200$）<br>做卷积filter=128,kernel=5 (卷积后维度为$600\\times12\\times128$)<br>沿行做最大池化，选择所有字符中最大的向量作为单词的最终字符向量。（维度变成$600\\times128$）</p>\n</blockquote>\n</li>\n<li><p>拼接，再经过两层Highway Network得到最终的单词向量表示，维度为$600\\times328$</p>\n</li>\n</ul>\n<h4 id=\"Embedding-Encoder层\"><a href=\"#Embedding-Encoder层\" class=\"headerlink\" title=\"Embedding Encoder层\"></a>Embedding Encoder层</h4><p>提取Context和query中的信息<br>每一个Encoder块是由卷积、Self-Attention、全连接层组成。<br>输入向量维数是d=328，输出d=128.<br><img src=\"https://ws2.sinaimg.cn/large/006tNc79ly1fvrpsx6edxj30k80xsmzy.jpg\" alt=\"image\"><br><strong>1. Position encoding</strong><br>纯Attention模型无法捕捉序列的顺序，如果将K,V按行打乱顺序（相当于打乱句子中的次序），Attention的结果还是一样的。如果没有序列顺序信息，Attention模型顶多是一个非常精妙的“词袋模型”而已。Position Embedding是位置信息的唯一来源。</p>\n<p>$PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})$<br>$PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})$<br>pos(i+k)可以表达成pos(i)的线性组合，对相对距离的表示非常有利<br>把输入加上position embedding的结果作为输出</p>\n<p><strong>2. 深度可分离卷积</strong><br>经典卷积：卷积核在所有输入通道上进行卷积，并综合所有输入通道情况得到卷积结果（池化）<br>深度可分离卷积：卷积核对每个输入通道分别做卷积，然后对得到的新的feature map先concat起来再使用$1 \\times 1$的卷积核将输出通道混合。<br><img src=\"https://ws1.sinaimg.cn/large/006tNc79ly1fvrpu79woaj310a0huq5v.jpg\" alt=\"image\"><br><img src=\"https://ws2.sinaimg.cn/large/006tNc79ly1fvrpulkoc4j30uy06gwfs.jpg\" alt=\"image\"></p>\n<p>优点：把空间特征学习和通道特征学习分开，这样可以提高泛化能力和卷积效率，避免参数冗余。（Xception的基础）</p>\n<p>用3个$3\\times3$（1通道）的卷积核分别与输入的3通道的数据做卷积，得到3个feature map，然后用256个$1 \\times 1$大小的卷积核（3通道）在这3个feature map上进行卷积运算，将3个通道的信息进行融合。</p>\n<p><strong>3. 自注意力机制</strong><br><a href=\"https://arxiv.org/pdf/1706.03762.pdf\" target=\"_blank\" rel=\"noopener\">Attention is All You Need</a><br>序列编码：RNN要逐步递归才能获得全局信息，因此一般要双向RNN才比较好；CNN事实上只能获取局部信息，是通过层叠来增大感受野；Attention的思路最为粗暴，它一步到位获取了全局信息。<br><img src=\"https://ws2.sinaimg.cn/large/006tNc79ly1fvrpvist54j30og0ss0uz.jpg\" alt=\"image\"><br>$Attention(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V}) = softmax\\left(\\frac{\\boldsymbol{Q}\\boldsymbol{K}^{\\top}}{\\sqrt{d_k}}\\right)\\boldsymbol{V}$<br>query和key做內积并softmax,来得到query和value之间的相似度，然后加权求和。</p>\n<p><strong>Self Attention</strong></p>\n<p>如果Q=K=V，那么就称为Self Attention，它的意思是直接将每个词与原来的每个词进行比较，计算出表示。也就是在序列内部做Attention，寻找序列内部的联系。</p>\n<p><strong>Muilti-Head Attention</strong></p>\n<p>这个是Google提出的新概念，是Attention机制的完善。不过从形式上看，它其实就再简单不过了，就是把Q,K,V通过参数矩阵映射一下，然后再做Attention，把这个过程重复做h次，结果拼接起来就行了<br><img src=\"https://ws1.sinaimg.cn/large/006tNc79ly1fvrpwc63ldj30hc0kkq4k.jpg\" alt=\"image\"></p>\n<p>所谓“多头”（Multi-Head），就是指多做几次同样的事情（参数不共享），然后把结果拼接。<br>一方面，从直觉上多次attention操作可以捕获更多的信息，另一方面，先进行的投影操作能把QKV映射到不同空间，也许能发现更多特征。</p>\n<p><strong>4. 层归一化+残差连接</strong></p>\n<p>在encoder block中，每个子层都用了layernorm和残差连接：<br>$Output = f(layernorm(x)) + x$<br>其中 f 表示encoder block 中的子层，如 depth conv, self-attention, feed-forward等。layernorm() 表示 layer normalization。</p>\n<p><img src=\"https://ws3.sinaimg.cn/large/006tNc79ly1fvrpwq23s1j314m0cs0up.jpg\" alt=\"image\"><br>BN：针对一个minibatch的输入样本，计算均值和方差，基于计算的均值和方差来对某一层神经网络的输入X中每一个case进行归一化操作。</p>\n<p>LN: 同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；而BN中则针对不同神经元输入计算均值和方差，同一个minibatch中的输入拥有相同的均值和方差。因此，LN不依赖于mini-batch的大小和输入sequence的深度，</p>\n<h4 id=\"Context-Query-Attention-Layer\"><a href=\"#Context-Query-Attention-Layer\" class=\"headerlink\" title=\"Context-Query Attention Layer\"></a>Context-Query Attention Layer</h4><p>发现context query 之间的联系，并在词的层面上，解析出query, context中关键的词语。</p>\n<ol>\n<li>首先计算context和query每对词之间的相似度，搞成一个相似度矩阵S，用的是BiDAF的算法：<br>$S_{i,j} = f(q,c ) = W_0[q,c,q\\odot c]$</li>\n<li>计算context-to-query的attention A：<br>$A = softmax(S, axis=row) \\cdot Q^T \\quad \\in R^{n\\times d}$</li>\n</ol>\n<blockquote>\n<p>实现中，context的长度是600，question长度是100，经过embedding encoder之后分别变成$S=600\\times128$和$Q=100\\times128$的表示。$S\\cdot Q^T$维度是$600\\times100$</p>\n</blockquote>\n<blockquote>\n<p>每行是一个context中的单词w，元素值是所有的query单词对于当前文档单词w的注意力分配权值<br>用$A$的每一行去乘以Q去表达单词w<br>这样就得到了用query表达context的结果</p>\n<ol start=\"3\">\n<li>计算query-to-context的attention B：<br>$B = A\\cdot softmax(S,axis=column)^T \\cdot C^T$<br>(这里采用了DCN的coattention) <a href=\"https://arxiv.org/abs/1611.01604\" target=\"_blank\" rel=\"noopener\">Dynamic Coattention Networks For Question Answering</a></li>\n</ol>\n</blockquote>\n<h4 id=\"Model-Encoder-layer\"><a href=\"#Model-Encoder-layer\" class=\"headerlink\" title=\"Model Encoder layer\"></a>Model Encoder layer</h4><p>从全局的层面来考虑context与query之间的关系。<br>输入是3个关于Context的矩阵信息：<br>原始Context：$C \\in \\mathcal{R}^{n\\times d}$<br>Context的Attention: $A \\in \\mathcal{R}^{n\\times d}$<br>Context的Coattention:$B \\in \\mathcal{R}^{n \\times d}$<br>每个单词的编码信息是上面三个矩阵的拼接<br>$f(w) = [c, a, c \\odot a, c \\odot b]$<br>一个有7个Encoder-Block，每个Encoder-Block：2个卷积层、Self-Attention、FFN。其它参数和Embedding Encoder一样。<br>一共有3个Model-Encoder，共享所有参数。输出依次为$M_0,M_1,M_2$.</p>\n<h4 id=\"Output-layer\"><a href=\"#Output-layer\" class=\"headerlink\" title=\"Output layer\"></a>Output layer</h4><p>解析answer在context中的位置<br>$pos^{start} = softmax(W_{start} [M_0; M_1]),\\quad  pos^{end} = softmax(W_{end}[M_0; M_2])$</p>\n<h4 id=\"Loss-function\"><a href=\"#Loss-function\" class=\"headerlink\" title=\"Loss function\"></a>Loss function</h4><p>$L(\\theta) = -  \\frac{1}{N}\\sum_{i}^N\\left[\\log(p_{y_i^{start}}^{start}) + \\log(p_{y_i^{end}}^{end})\\right]$</p>\n","site":{"data":{}},"excerpt":"<h2 id=\"QANet\"><a href=\"#QANet\" class=\"headerlink\" title=\"QANet\"></a>QANet</h2><h4 id=\"QANet-Combining-Local-Convolution-with-Global-Self-Attention-for-Reading-Comprehension\"><a href=\"#QANet-Combining-Local-Convolution-with-Global-Self-Attention-for-Reading-Comprehension\" class=\"headerlink\" title=\"QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension\"></a>QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension</h4><h3 id=\"特点\"><a href=\"#特点\" class=\"headerlink\" title=\"特点\"></a>特点</h3><ol>\n<li>移除RNN</li>\n<li>使用卷积捕捉局部信息</li>\n<li>自注意力机制捕捉全局信息</li>\n</ol>\n<h3 id=\"模型结构\"><a href=\"#模型结构\" class=\"headerlink\" title=\"模型结构\"></a>模型结构</h3>","more":"<p><img src=\"https://ws1.sinaimg.cn/large/006tNc79ly1fvrpc8wjfzj30rk0zw43g.jpg\" alt=\"image\"></p>\n<ol>\n<li>Embedding层</li>\n<li>Embedding Encoder层</li>\n<li>Context-Query Attention层</li>\n<li>Model encoder层</li>\n<li>Output层</li>\n</ol>\n<h4 id=\"Embedding层\"><a href=\"#Embedding层\" class=\"headerlink\" title=\"Embedding层\"></a>Embedding层</h4><p>利用词向量和字向量拼接的方式获得最终的词向量：<br>文章最大长度设为600</p>\n<ul>\n<li>词向量<blockquote>\n<p>预训练好的，300维的Glove<br>embedding后维度为$600\\times300$</p>\n</blockquote>\n</li>\n<li><p>字向量：</p>\n<blockquote>\n<p>预处理时将每个单词截断或者是padding到16个字符<br>每个字符表示成200维的向量 （embedding后维度变成$600\\times16\\times200$）<br>做卷积filter=128,kernel=5 (卷积后维度为$600\\times12\\times128$)<br>沿行做最大池化，选择所有字符中最大的向量作为单词的最终字符向量。（维度变成$600\\times128$）</p>\n</blockquote>\n</li>\n<li><p>拼接，再经过两层Highway Network得到最终的单词向量表示，维度为$600\\times328$</p>\n</li>\n</ul>\n<h4 id=\"Embedding-Encoder层\"><a href=\"#Embedding-Encoder层\" class=\"headerlink\" title=\"Embedding Encoder层\"></a>Embedding Encoder层</h4><p>提取Context和query中的信息<br>每一个Encoder块是由卷积、Self-Attention、全连接层组成。<br>输入向量维数是d=328，输出d=128.<br><img src=\"https://ws2.sinaimg.cn/large/006tNc79ly1fvrpsx6edxj30k80xsmzy.jpg\" alt=\"image\"><br><strong>1. Position encoding</strong><br>纯Attention模型无法捕捉序列的顺序，如果将K,V按行打乱顺序（相当于打乱句子中的次序），Attention的结果还是一样的。如果没有序列顺序信息，Attention模型顶多是一个非常精妙的“词袋模型”而已。Position Embedding是位置信息的唯一来源。</p>\n<p>$PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})$<br>$PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})$<br>pos(i+k)可以表达成pos(i)的线性组合，对相对距离的表示非常有利<br>把输入加上position embedding的结果作为输出</p>\n<p><strong>2. 深度可分离卷积</strong><br>经典卷积：卷积核在所有输入通道上进行卷积，并综合所有输入通道情况得到卷积结果（池化）<br>深度可分离卷积：卷积核对每个输入通道分别做卷积，然后对得到的新的feature map先concat起来再使用$1 \\times 1$的卷积核将输出通道混合。<br><img src=\"https://ws1.sinaimg.cn/large/006tNc79ly1fvrpu79woaj310a0huq5v.jpg\" alt=\"image\"><br><img src=\"https://ws2.sinaimg.cn/large/006tNc79ly1fvrpulkoc4j30uy06gwfs.jpg\" alt=\"image\"></p>\n<p>优点：把空间特征学习和通道特征学习分开，这样可以提高泛化能力和卷积效率，避免参数冗余。（Xception的基础）</p>\n<p>用3个$3\\times3$（1通道）的卷积核分别与输入的3通道的数据做卷积，得到3个feature map，然后用256个$1 \\times 1$大小的卷积核（3通道）在这3个feature map上进行卷积运算，将3个通道的信息进行融合。</p>\n<p><strong>3. 自注意力机制</strong><br><a href=\"https://arxiv.org/pdf/1706.03762.pdf\" target=\"_blank\" rel=\"noopener\">Attention is All You Need</a><br>序列编码：RNN要逐步递归才能获得全局信息，因此一般要双向RNN才比较好；CNN事实上只能获取局部信息，是通过层叠来增大感受野；Attention的思路最为粗暴，它一步到位获取了全局信息。<br><img src=\"https://ws2.sinaimg.cn/large/006tNc79ly1fvrpvist54j30og0ss0uz.jpg\" alt=\"image\"><br>$Attention(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V}) = softmax\\left(\\frac{\\boldsymbol{Q}\\boldsymbol{K}^{\\top}}{\\sqrt{d_k}}\\right)\\boldsymbol{V}$<br>query和key做內积并softmax,来得到query和value之间的相似度，然后加权求和。</p>\n<p><strong>Self Attention</strong></p>\n<p>如果Q=K=V，那么就称为Self Attention，它的意思是直接将每个词与原来的每个词进行比较，计算出表示。也就是在序列内部做Attention，寻找序列内部的联系。</p>\n<p><strong>Muilti-Head Attention</strong></p>\n<p>这个是Google提出的新概念，是Attention机制的完善。不过从形式上看，它其实就再简单不过了，就是把Q,K,V通过参数矩阵映射一下，然后再做Attention，把这个过程重复做h次，结果拼接起来就行了<br><img src=\"https://ws1.sinaimg.cn/large/006tNc79ly1fvrpwc63ldj30hc0kkq4k.jpg\" alt=\"image\"></p>\n<p>所谓“多头”（Multi-Head），就是指多做几次同样的事情（参数不共享），然后把结果拼接。<br>一方面，从直觉上多次attention操作可以捕获更多的信息，另一方面，先进行的投影操作能把QKV映射到不同空间，也许能发现更多特征。</p>\n<p><strong>4. 层归一化+残差连接</strong></p>\n<p>在encoder block中，每个子层都用了layernorm和残差连接：<br>$Output = f(layernorm(x)) + x$<br>其中 f 表示encoder block 中的子层，如 depth conv, self-attention, feed-forward等。layernorm() 表示 layer normalization。</p>\n<p><img src=\"https://ws3.sinaimg.cn/large/006tNc79ly1fvrpwq23s1j314m0cs0up.jpg\" alt=\"image\"><br>BN：针对一个minibatch的输入样本，计算均值和方差，基于计算的均值和方差来对某一层神经网络的输入X中每一个case进行归一化操作。</p>\n<p>LN: 同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；而BN中则针对不同神经元输入计算均值和方差，同一个minibatch中的输入拥有相同的均值和方差。因此，LN不依赖于mini-batch的大小和输入sequence的深度，</p>\n<h4 id=\"Context-Query-Attention-Layer\"><a href=\"#Context-Query-Attention-Layer\" class=\"headerlink\" title=\"Context-Query Attention Layer\"></a>Context-Query Attention Layer</h4><p>发现context query 之间的联系，并在词的层面上，解析出query, context中关键的词语。</p>\n<ol>\n<li>首先计算context和query每对词之间的相似度，搞成一个相似度矩阵S，用的是BiDAF的算法：<br>$S_{i,j} = f(q,c ) = W_0[q,c,q\\odot c]$</li>\n<li>计算context-to-query的attention A：<br>$A = softmax(S, axis=row) \\cdot Q^T \\quad \\in R^{n\\times d}$</li>\n</ol>\n<blockquote>\n<p>实现中，context的长度是600，question长度是100，经过embedding encoder之后分别变成$S=600\\times128$和$Q=100\\times128$的表示。$S\\cdot Q^T$维度是$600\\times100$</p>\n</blockquote>\n<blockquote>\n<p>每行是一个context中的单词w，元素值是所有的query单词对于当前文档单词w的注意力分配权值<br>用$A$的每一行去乘以Q去表达单词w<br>这样就得到了用query表达context的结果</p>\n<ol start=\"3\">\n<li>计算query-to-context的attention B：<br>$B = A\\cdot softmax(S,axis=column)^T \\cdot C^T$<br>(这里采用了DCN的coattention) <a href=\"https://arxiv.org/abs/1611.01604\" target=\"_blank\" rel=\"noopener\">Dynamic Coattention Networks For Question Answering</a></li>\n</ol>\n</blockquote>\n<h4 id=\"Model-Encoder-layer\"><a href=\"#Model-Encoder-layer\" class=\"headerlink\" title=\"Model Encoder layer\"></a>Model Encoder layer</h4><p>从全局的层面来考虑context与query之间的关系。<br>输入是3个关于Context的矩阵信息：<br>原始Context：$C \\in \\mathcal{R}^{n\\times d}$<br>Context的Attention: $A \\in \\mathcal{R}^{n\\times d}$<br>Context的Coattention:$B \\in \\mathcal{R}^{n \\times d}$<br>每个单词的编码信息是上面三个矩阵的拼接<br>$f(w) = [c, a, c \\odot a, c \\odot b]$<br>一个有7个Encoder-Block，每个Encoder-Block：2个卷积层、Self-Attention、FFN。其它参数和Embedding Encoder一样。<br>一共有3个Model-Encoder，共享所有参数。输出依次为$M_0,M_1,M_2$.</p>\n<h4 id=\"Output-layer\"><a href=\"#Output-layer\" class=\"headerlink\" title=\"Output layer\"></a>Output layer</h4><p>解析answer在context中的位置<br>$pos^{start} = softmax(W_{start} [M_0; M_1]),\\quad  pos^{end} = softmax(W_{end}[M_0; M_2])$</p>\n<h4 id=\"Loss-function\"><a href=\"#Loss-function\" class=\"headerlink\" title=\"Loss function\"></a>Loss function</h4><p>$L(\\theta) = -  \\frac{1}{N}\\sum_{i}^N\\left[\\log(p_{y_i^{start}}^{start}) + \\log(p_{y_i^{end}}^{end})\\right]$</p>"},{"title":"Hello World","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","slug":"hello-world","published":1,"date":"2018-09-30T12:31:24.249Z","updated":"2018-09-30T12:31:24.249Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjmox84s2000124oscfk0lh8j","content":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"noopener\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"noopener\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"noopener\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"noopener\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"noopener\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"noopener\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"noopener\">Deployment</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"noopener\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"noopener\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"noopener\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"noopener\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"noopener\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"noopener\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"noopener\">Deployment</a></p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cjmox84rx000024os0usa6hjt","category_id":"cjmox84s4000224osbt8hns59","_id":"cjmox84s9000824os0kad6kzt"},{"post_id":"cjmox84rx000024os0usa6hjt","category_id":"cjmox84s7000524osgnyj2ic5","_id":"cjmox84sa000924ost9b3ng4r"}],"PostTag":[{"post_id":"cjmox84rx000024os0usa6hjt","tag_id":"cjmox84s6000324osv4jq5r05","_id":"cjmox84s8000624osb3d1x7sk"},{"post_id":"cjmox84rx000024os0usa6hjt","tag_id":"cjmox84s7000424osxqpsmxpr","_id":"cjmox84s9000724oshqu6k49m"}],"Tag":[{"name":"NLP","_id":"cjmox84s6000324osv4jq5r05"},{"name":"machine-reading-comprehension","_id":"cjmox84s7000424osxqpsmxpr"}]}}